{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Workflow\n",
    "Notes: \n",
    "* Continued issues accessing articles behind a paywall or external source (e.g., https://finance.yahoo.com/news/top-midday-stories-pepsico-buy-160405890.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import yfinance as yf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# TODO: Compare using requests vs using drivers\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from seleniumbase import Driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get website html data \n",
    "def get_basesoup(driver, url, wait=False, until_class='ClassOfMyElement'):\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for page and element to completely load\n",
    "    if wait:\n",
    "        delay = 3 # seconds\n",
    "        try:\n",
    "            WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CLASS_NAME, until_class)))\n",
    "        except TimeoutException:\n",
    "            print(\"Loading took too much time!\")\n",
    "    \n",
    "    basesoup=BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "    return basesoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_info(url):\n",
    "    '''\n",
    "    Return a list of strings for a given url, \n",
    "    where each string is a sentence in the linked article.\n",
    "    '''\n",
    "    soup = get_basesoup(driver, url)\n",
    "\n",
    "    try:\n",
    "        article = soup.find(\"div\", class_=\"article-wrap no-bb\")\n",
    "        \n",
    "        if not article:\n",
    "            print(f\"No articles found on page {url}\")\n",
    "            return []\n",
    "\n",
    "        # cover_wrap = article.find(\"div\", class_=\"cover-wrap yf-1p8y0lh\")\n",
    "        # title = cover_wrap.find(\"h1\", class_=\"cover-title yf-1p8y0lh\")\n",
    "             \n",
    "        body_wrap = article.find(\"div\", class_=\"body-wrap yf-i23rhs\")\n",
    "        body = body_wrap.find(\"div\", class_=\"body yf-5ef8bf\")\n",
    "        text = body.find_all(\"p\", class_=\"yf-1pe5jgt\")\n",
    "        # for paragraph in text:\n",
    "        #     print(paragraph.text.strip())\n",
    "        \n",
    "        return [paragraph.text.strip() for paragraph in text]\n",
    "            \n",
    "        \n",
    "    except:\n",
    "        print(f\"Error accessing articles on page {url}\")\n",
    "        return []\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_yfinance(ticker):\n",
    "    news = yf.Ticker(ticker).news\n",
    "    urls = {dictionary['link'] for dictionary in news}\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape all related articles\n",
    "def get_list_all_articles_text_data(urls):\n",
    "    article_texts = []\n",
    "    \n",
    "    for url in urls:\n",
    "        article_texts.append(get_news_info(url)) #webscraping step\n",
    "\n",
    "    return article_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sentences(text_data):\n",
    "    output_text = \"\"\n",
    "    for sentence in text_data:\n",
    "        output_text += \" \" + sentence\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_finbert(text_data):\n",
    "    text = combine_sentences(text_data)\n",
    "    tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "    model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the logits (raw output predictions)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Convert logits to probabilities using softmax\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Extract probabilities and predicted sentiment class\n",
    "    predicted_class = torch.argmax(probs).item()  # 0: negative, 1: neutral, 2: positive\n",
    "    confidence = torch.max(probs).item()\n",
    "\n",
    "    # Sentiment mapping\n",
    "    sentiment_labels = ['Negative', 'Neutral', 'Positive']\n",
    "    \n",
    "    # Get the predicted sentiment label\n",
    "    predicted_sentiment = sentiment_labels[predicted_class]\n",
    "    \n",
    "    # Set a confidence threshold (e.g., 70%)\n",
    "    confidence_threshold = 0.7\n",
    "    \n",
    "    # Output result with a check on confidence\n",
    "    if confidence >= confidence_threshold:\n",
    "        print(f\"Sentiment: {predicted_sentiment} (Confidence: {confidence:.2f})\")\n",
    "    else:\n",
    "        print(\"Sentiment prediction not reliable enough based on confidence threshold.\")\n",
    "    return [predicted_sentiment, confidence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_scores_finbert(article_texts):\n",
    "    cnt_neu = 0\n",
    "    cnt_pos = 0\n",
    "    cnt_neg = 0\n",
    "    total_confidence = 0\n",
    "    for text in article_texts:\n",
    "        sentiment, confidence = use_finbert(text)\n",
    "        #print(text)\n",
    "        if sentiment == \"Neutral\":\n",
    "            cnt_neu += 1\n",
    "        elif sentiment == \"Positive\":\n",
    "            cnt_pos += 1\n",
    "        elif sentiment == \"Negative\":\n",
    "            cnt_neg += 1\n",
    "        total_confidence += confidence\n",
    "    return [cnt_neu, cnt_pos, cnt_neg, total_confidence/len(article_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import google.generativeai as genai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gemini_sentiment_score_one_article(api_key_gemini, text):\n",
    "    genai.configure(api_key=api_key_gemini)\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    response = model.generate_content(\"Please conduct sentiment analysis on the following articles of interest. Here is the text: \"+text+\n",
    "                                  '''Output text should be in JSON format with no extra information or text. Do NOT include extra formatting.\n",
    "                                  Your response should start with { and end with }. Do not include `.\n",
    "                                  Include categories neutral-sentiment,''' +\n",
    "                                  \"positive-sentiment, negative-sentiment, summary and stock-tickers. The sentiment categories should include \"+\n",
    "                                  \"an integer from 0 to 9, where 0 means that the text doesn't fit that category and 9 means it fits well.\" +\n",
    "                                  \"The summary should be a once-sentence summary about the text. Stock-tickers should be the tickers of stocks related\" +\n",
    "                                  \"to the articles.\" +\n",
    "                                  '''If no article is given, output an empty \n",
    "                                  json \"{}\" only. Here is an example of required formatting: ''' +\n",
    "\n",
    "                                  ''' {\"neutral-sentiment\": #,\n",
    "                                    \"npositive-sentiment\": #,\n",
    "                                    \"negative-sentiment\": #,\n",
    "                                    \"related-stocks\": [\"ABC\", \"DEF\", \"GHI\"] +\n",
    "                                    \"nsummary : \"Include a 2-sentence summary of the article text here.\" +\n",
    "                                    }''')\n",
    "    response_string = response.text\n",
    "    if response_string == \"{}\":\n",
    "        return None\n",
    "    try:\n",
    "        response_json = json.loads(response_string)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "    return response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_gemini_sentiment_scores(article_texts):\n",
    "    avg_neu = 0\n",
    "    avg_pos = 0\n",
    "    avg_neg = 0\n",
    "    api_key_gemini= \"AIzaSyAqV8jVx8ah9Iv_04t1NcGdCjQoaAm8Uu4\"\n",
    "    num_articles = len(article_texts)\n",
    "    for article in article_texts:\n",
    "        text_data = combine_sentences(article)\n",
    "        sentiment_scores = get_gemini_sentiment_score_one_article(api_key_gemini, text_data)\n",
    "        if not sentiment_scores:\n",
    "            continue\n",
    "        else:\n",
    "            avg_neu += sentiment_scores[\"neutral-sentiment\"]\n",
    "            avg_pos += sentiment_scores[\"positive-sentiment\"]\n",
    "            avg_neg += sentiment_scores[\"negative-sentiment\"]\n",
    "    return [avg_neu/num_articles, avg_pos/num_articles, avg_neg/num_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look into: Long Short Term Memory (LSTM) algorithm\n",
    "#https://www.nature.com/articles/s41599-024-02807-x\n",
    "#running window (e,g, 5 or 10) -> prediction is following day\n",
    "#prediction could be 1 day after or 1 month?\n",
    "\n",
    "#making dataframe\n",
    "\n",
    "#stable and larger companies features (maybe 2 extra columns)\n",
    "#confidence data\n",
    "#storing in a database (at a later point)\n",
    "#validating model ourselves\n",
    "#real-time data might be harder to use (delay)\n",
    "\n",
    "#actually apply resulting model\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe(ticker):\n",
    "    data = yf.download(ticker, start = datetime.now(), end = datetime.now())\n",
    "    df = pd.DataFrame(data)\n",
    "    urls = get_urls_yfinance(ticker)\n",
    "    article_texts = get_list_all_articles_text_data(urls)\n",
    "    \n",
    "    sentiment_labels_finbert = [\"neutral-count-finbert\", \"positive-count-finbert\",\"negative-count-finbert\",\"average-confidence-finbert\"]\n",
    "    sentiment_scores_finbert = get_sentiment_scores_finbert(article_texts)\n",
    "    for i in range(len(sentiment_labels_finbert)):\n",
    "      df[sentiment_labels_finbert[i]] = sentiment_scores_finbert[i]\n",
    "\n",
    "    sentiment_labels_gemini = [\"average-neutral-score-gemini\", \"average-positive-score-gemini\",\"average-negative-score-gemini\"]\n",
    "    sentiment_scores_gemini = get_all_gemini_sentiment_scores(article_texts)\n",
    "    for i in range(len(sentiment_labels_gemini)):\n",
    "        df[sentiment_labels_gemini[i]] = sentiment_scores_gemini[i]\n",
    "        \n",
    "    df[\"prediction-label\"] = ''\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db():\n",
    "    connection_string = \"mongodb+srv://varshaathreya:P9OTU6PVHDG1CITH@cluster0.luavu.mongodb.net/\"\n",
    "\n",
    "    # Step 1: Connect to MongoDB Atlas\n",
    "    client = MongoClient(connection_string)\n",
    "    \n",
    "    # Step 2: Select the database and collection\n",
    "    db = client[\"predictive-analysis-dataset\"]  # Replace with your database name\n",
    "    collection = db[\"stocks\"]  # Replace with your collection name\n",
    "    \n",
    "    # Step 3: Read CSV into a DataFrame\n",
    "    csv_file_path = \"stock_data.csv\"  # Path to your CSV file\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Step 4: Convert DataFrame to List of Dictionaries\n",
    "    data = df.to_dict(orient=\"records\")  # Converts rows into a list of dictionaries\n",
    "    \n",
    "    # Step 5: Insert data into MongoDB Atlas collection\n",
    "    collection.insert_many(data)\n",
    "    \n",
    "    print(\"Data successfully imported to MongoDB Atlas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up web driver\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = Driver(uc=True, incognito=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_dataframe('TSLA') #only necessary if stock_data file has not been created\n",
    "stock_ticker_list = ['TSLA', 'AAPL', 'LCID', 'PFE', 'VZ', 'NVDA', 'JNJ', 'T', 'RTX', 'MDT', 'GOOGL', 'BSX', 'META']\n",
    "for stock in stock_ticker_list:\n",
    "    curr_df = make_dataframe(stock)\n",
    "    curr_df.to_csv('stock_data.csv', mode = 'a', header = False)\n",
    "update_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
