{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Workflow\n",
    "Notes: \n",
    "* Continued issues accessing articles behind a paywall or external source (e.g., https://finance.yahoo.com/news/top-midday-stories-pepsico-buy-160405890.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import yfinance as yf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# TODO: Compare using requests vs using drivers\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from seleniumbase import Driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get website html data \n",
    "def get_basesoup(driver, url, wait=False, until_class='ClassOfMyElement'):\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for page and element to completely load\n",
    "    if wait:\n",
    "        delay = 3 # seconds\n",
    "        try:\n",
    "            WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CLASS_NAME, until_class)))\n",
    "        except TimeoutException:\n",
    "            print(\"Loading took too much time!\")\n",
    "    \n",
    "    basesoup=BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "    return basesoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_info(url):\n",
    "    '''\n",
    "    Return a list of strings for a given url, \n",
    "    where each string is a sentence in the linked article.\n",
    "    '''\n",
    "    soup = get_basesoup(driver, url)\n",
    "\n",
    "    try:\n",
    "        article = soup.find(\"div\", class_=\"article-wrap no-bb\")\n",
    "        \n",
    "        if not article:\n",
    "            print(f\"No articles found on page {url}\")\n",
    "            return []\n",
    "\n",
    "        # cover_wrap = article.find(\"div\", class_=\"cover-wrap yf-1p8y0lh\")\n",
    "        # title = cover_wrap.find(\"h1\", class_=\"cover-title yf-1p8y0lh\")\n",
    "             \n",
    "        body_wrap = article.find(\"div\", class_=\"body-wrap yf-i23rhs\")\n",
    "        body = body_wrap.find(\"div\", class_=\"body yf-5ef8bf\")\n",
    "        text = body.find_all(\"p\", class_=\"yf-1pe5jgt\")\n",
    "        # for paragraph in text:\n",
    "        #     print(paragraph.text.strip())\n",
    "        \n",
    "        return [paragraph.text.strip() for paragraph in text]\n",
    "            \n",
    "        \n",
    "    except:\n",
    "        print(f\"Error accessing articles on page {url}\")\n",
    "        return []\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_yfinance(ticker):\n",
    "    news = yf.Ticker(ticker).news\n",
    "    urls = {dictionary['link'] for dictionary in news}\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape all related articles\n",
    "def get_list_all_articles_text_data(urls):\n",
    "    article_texts = []\n",
    "    \n",
    "    for url in urls:\n",
    "        article_texts.append(get_news_info(url)) #webscraping step\n",
    "\n",
    "    return article_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sentences(text_data):\n",
    "    output_text = \"\"\n",
    "    for sentence in text_data:\n",
    "        output_text += \" \" + sentence\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_finbert(text_data):\n",
    "    text = combine_sentences(text_data)\n",
    "    tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "    model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the logits (raw output predictions)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Convert logits to probabilities using softmax\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Extract probabilities and predicted sentiment class\n",
    "    predicted_class = torch.argmax(probs).item()  # 0: negative, 1: neutral, 2: positive\n",
    "    confidence = torch.max(probs).item()\n",
    "\n",
    "    # Sentiment mapping\n",
    "    sentiment_labels = ['Negative', 'Neutral', 'Positive']\n",
    "    \n",
    "    # Get the predicted sentiment label\n",
    "    predicted_sentiment = sentiment_labels[predicted_class]\n",
    "    \n",
    "    # Set a confidence threshold (e.g., 70%)\n",
    "    confidence_threshold = 0.7\n",
    "    \n",
    "    # Output result with a check on confidence\n",
    "    if confidence >= confidence_threshold:\n",
    "        print(f\"Sentiment: {predicted_sentiment} (Confidence: {confidence:.2f})\")\n",
    "    else:\n",
    "        print(\"Sentiment prediction not reliable enough based on confidence threshold.\")\n",
    "    return [predicted_sentiment, confidence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_scores_finbert(article_texts):\n",
    "    cnt_neu = 0\n",
    "    cnt_pos = 0\n",
    "    cnt_neg = 0\n",
    "    total_confidence = 0\n",
    "    for text in article_texts:\n",
    "        sentiment, confidence = use_finbert(text)\n",
    "        #print(text)\n",
    "        if sentiment == \"Neutral\":\n",
    "            cnt_neu += 1\n",
    "        elif sentiment == \"Positive\":\n",
    "            cnt_pos += 1\n",
    "        elif sentiment == \"Negative\":\n",
    "            cnt_neg += 1\n",
    "        total_confidence += confidence\n",
    "    return [cnt_neu, cnt_pos, cnt_neg, total_confidence/len(article_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import google.generativeai as genai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gemini_sentiment_score_one_article(api_key_gemini, text):\n",
    "    genai.configure(api_key=api_key_gemini)\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    response = model.generate_content(\"Please conduct sentiment analysis on the following articles of interest. Here is the text: \"+text+\n",
    "                                  '''Output text should be in JSON format with no extra information or text. Do NOT include extra formatting.\n",
    "                                  Your response should start with { and end with }. Do not include `.\n",
    "                                  Include categories neutral-sentiment,''' +\n",
    "                                  \"positive-sentiment, negative-sentiment, summary and stock-tickers. The sentiment categories should include \"+\n",
    "                                  \"an integer from 0 to 9, where 0 means that the text doesn't fit that category and 9 means it fits well.\" +\n",
    "                                  \"The summary should be a once-sentence summary about the text. Stock-tickers should be the tickers of stocks related\" +\n",
    "                                  \"to the articles.\" +\n",
    "                                  '''If no article is given, output an empty \n",
    "                                  json \"{}\" only. Here is an example of required formatting: ''' +\n",
    "\n",
    "                                  ''' {\"neutral-sentiment\": #,\n",
    "                                    \"npositive-sentiment\": #,\n",
    "                                    \"negative-sentiment\": #,\n",
    "                                    \"related-stocks\": [\"ABC\", \"DEF\", \"GHI\"] +\n",
    "                                    \"nsummary : \"Include a 2-sentence summary of the article text here.\" +\n",
    "                                    }''')\n",
    "    response_string = response.text\n",
    "    if response_string == \"{}\":\n",
    "        return None\n",
    "    try:\n",
    "        response_json = json.loads(response_string)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "    return response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_gemini_sentiment_scores(article_texts):\n",
    "    avg_neu = 0\n",
    "    avg_pos = 0\n",
    "    avg_neg = 0\n",
    "    api_key_gemini= \"AIzaSyAqV8jVx8ah9Iv_04t1NcGdCjQoaAm8Uu4\"\n",
    "    num_articles = len(article_texts)\n",
    "    for article in article_texts:\n",
    "        text_data = combine_sentences(article)\n",
    "        sentiment_scores = get_gemini_sentiment_score_one_article(api_key_gemini, text_data)\n",
    "        if not sentiment_scores:\n",
    "            continue\n",
    "        else:\n",
    "            avg_neu += sentiment_scores[\"neutral-sentiment\"]\n",
    "            avg_pos += sentiment_scores[\"positive-sentiment\"]\n",
    "            avg_neg += sentiment_scores[\"negative-sentiment\"]\n",
    "    return [avg_neu/num_articles, avg_pos/num_articles, avg_neg/num_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look into: Long Short Term Memory (LSTM) algorithm\n",
    "#https://www.nature.com/articles/s41599-024-02807-x\n",
    "#running window (e,g, 5 or 10) -> prediction is following day\n",
    "#prediction could be 1 day after or 1 month?\n",
    "\n",
    "#making dataframe\n",
    "\n",
    "#stable and larger companies features (maybe 2 extra columns)\n",
    "#confidence data\n",
    "#storing in a database (at a later point)\n",
    "#validating model ourselves\n",
    "#real-time data might be harder to use (delay)\n",
    "\n",
    "#actually apply resulting model\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe(ticker):\n",
    "    data = yf.download(ticker, start = datetime.now(), end = datetime.now())\n",
    "    df = pd.DataFrame(data)\n",
    "    urls = get_urls_yfinance(ticker)\n",
    "    article_texts = get_list_all_articles_text_data(urls)\n",
    "    \n",
    "    sentiment_labels_finbert = [\"neutral-count-finbert\", \"positive-count-finbert\",\"negative-count-finbert\",\"average-confidence-finbert\"]\n",
    "    sentiment_scores_finbert = get_sentiment_scores_finbert(article_texts)\n",
    "    for i in range(len(sentiment_labels_finbert)):\n",
    "      df[sentiment_labels_finbert[i]] = sentiment_scores_finbert[i]\n",
    "\n",
    "    sentiment_labels_gemini = [\"average-neutral-score-gemini\", \"average-positive-score-gemini\",\"average-negative-score-gemini\"]\n",
    "    sentiment_scores_gemini = get_all_gemini_sentiment_scores(article_texts)\n",
    "    for i in range(len(sentiment_labels_gemini)):\n",
    "        df[sentiment_labels_gemini[i]] = sentiment_scores_gemini[i]\n",
    "        \n",
    "    df[\"prediction-label\"] = ''\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db():\n",
    "    connection_string = \"mongodb+srv://varshaathreya:P9OTU6PVHDG1CITH@cluster0.luavu.mongodb.net/\"\n",
    "\n",
    "    # Step 1: Connect to MongoDB Atlas\n",
    "    client = MongoClient(connection_string)\n",
    "    \n",
    "    # Step 2: Select the database and collection\n",
    "    db = client[\"predictive-analysis-dataset\"]  # Replace with your database name\n",
    "    collection = db[\"stocks\"]  # Replace with your collection name\n",
    "    \n",
    "    # Step 3: Read CSV into a DataFrame\n",
    "    csv_file_path = \"stock_data.csv\"  # Path to your CSV file\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Step 4: Convert DataFrame to List of Dictionaries\n",
    "    data = df.to_dict(orient=\"records\")  # Converts rows into a list of dictionaries\n",
    "    \n",
    "    # Step 5: Insert data into MongoDB Atlas collection\n",
    "    collection.insert_many(data)\n",
    "    \n",
    "    print(\"Data successfully imported to MongoDB Atlas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up web driver\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = Driver(uc=True, incognito=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_dataframe('TSLA') #only necessary if stock_data file has not been created\n",
    "stock_ticker_list = ['TSLA', 'AAPL', 'LCID', 'PFE', 'VZ', 'NVDA', 'JNJ', 'T', 'RTX', 'MDT', 'GOOGL', 'BSX', 'META']\n",
    "for stock in stock_ticker_list:\n",
    "    curr_df = make_dataframe(stock)\n",
    "    curr_df.to_csv('stock_data.csv', mode = 'a', header = False)\n",
    "update_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (2.1.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 51731.2656\n",
      "Epoch [20/100], Loss: 51615.4414\n",
      "Epoch [30/100], Loss: 51527.5703\n",
      "Epoch [40/100], Loss: 51452.8320\n",
      "Epoch [50/100], Loss: 51378.1016\n",
      "Epoch [60/100], Loss: 51303.8242\n",
      "Epoch [70/100], Loss: 51230.0195\n",
      "Epoch [80/100], Loss: 51156.6211\n",
      "Epoch [90/100], Loss: 51083.5586\n",
      "Epoch [100/100], Loss: 51010.7734\n",
      "Prediction: 0.3531138300895691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kte123/Documents/BreakthroughTechAI/Predictive-Analysis/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Define a fully connected (linear) layer for output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM outputs\n",
    "        lstm_out, (hn, cn) = self.lstm(x)  # hn is the hidden state from the last LSTM layer\n",
    "        \n",
    "        # We take the output from the last time step\n",
    "        out = self.fc(hn[-1])  # Get the last hidden state from the last layer\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 8  # Number of features in each time step (can vary based on your dataset)\n",
    "hidden_size = 50  # Number of LSTM units in each layer\n",
    "output_size = 1  # For regression, change this for classification (e.g., number of classes)\n",
    "num_layers = 1  # Number of LSTM layers\n",
    "batch_size = 32\n",
    "seq_length = 20  # Length of each input sequence\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Generate some synthetic data for demonstration (e.g., time-series regression)\n",
    "# Let's generate a random sequence with 1000 samples of length 20\n",
    "df = pd.read_csv('stock_data.csv')\n",
    "df = df[df[\"ticker\"]==\"AAPL\"]\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Convert data into sequences for LSTM input\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data.iloc[i:i+seq_length, 0].values  # Accessing the first column (numerical data)\n",
    "        y = data.iloc[i+seq_length, 0]  # The value at the next timestep\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# Set sequence length\n",
    "SEQ_LENGTH = 8\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences(df, SEQ_LENGTH)\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMModel(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Using Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(X_train)  # Get model predictions\n",
    "    loss = criterion(outputs, y_train)  # Calculate the loss\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()  # Compute gradients\n",
    "    optimizer.step()  # Update model parameters\n",
    "    \n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "# After training, you can make predictions like this:\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    test_input = torch.randn(1, seq_length, input_size)  # A single sample\n",
    "    prediction = model(test_input)\n",
    "    print(\"Prediction:\", prediction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
